{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier, export_text\n",
    "import pickle\n",
    "from scipy.stats import qmc\n",
    "import gc\n",
    "import tracemalloc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import os"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1: Load the training data, train the Neural Network and save the following:\n",
    "1. model\n",
    "2. feature names\n",
    "3. the scaler used"
   ],
   "id": "931e778dfcd0b8d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Loading and processing training data\")\n",
    "df_train = pd.read_csv(\"Steganography_Dataset/features_train_70000.csv\")\n",
    "df_train.columns = df_train.columns.str.strip()\n",
    "print(\"Training data loaded. Shape: \", df_train.shape)\n",
    "\n",
    "def parse_complex(x):\n",
    "    if isinstance(x, str) and 'i' in x:\n",
    "        x = x.replace('i', 'j')\n",
    "        try:\n",
    "            return abs(complex(x))\n",
    "        except:\n",
    "            return np.nan\n",
    "    return x\n",
    "\n",
    "df_train = df_train.apply(lambda col: col.map(parse_complex))\n",
    "df_train = df_train.dropna()\n",
    "\n",
    "X_train = df_train.drop(\"Tag\", axis=1).astype(float).values\n",
    "y_train = df_train[\"Tag\"].astype(float).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "with open('scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "print(\"Scaler saved to 'scaler.pkl' for future use\")\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)"
   ],
   "id": "cfac27926f2f967b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining the Neural Network Architecture",
   "id": "159f0a35f44c6360"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "model = Net(X_train_tensor.shape[1])\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "ea8b4d05d264145a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train the Neural Network and Save the model and feature names.",
   "id": "ceb5aee6c813feb6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"\\nTraining Neural Network\")\n",
    "epochs = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "torch.save(model, 'neural_network_model.pth')\n",
    "print(\"Model saved to 'neural_network_model.pth'\")\n",
    "\n",
    "feature_names = df_train.drop(\"Tag\", axis=1).columns.tolist()\n",
    "with open('feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(\"'feature_names.pkl' saved for future use\")"
   ],
   "id": "ba5d2d22aecafa7e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test Neural Network on test dataset",
   "id": "8e2c96b353027ac8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_test = pd.read_csv(\"Steganography_Dataset/features_test_70000.csv\")\n",
    "df_test.columns = df_test.columns.str.strip()\n",
    "print(\"Test data loaded. Shape:\", df_test.shape)\n",
    "\n",
    "# Use same parse_complex and apply lambda map\n",
    "df_test = df_test.apply(lambda col: col.map(parse_complex))\n",
    "df_test = df_test.dropna()\n",
    "\n",
    "X_test = df_test.drop(\"Tag\", axis=1).astype(float).values\n",
    "y_test = df_test[\"Tag\"].astype(float).values\n",
    "\n",
    "# Scale test data using saved scaler\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_preds = (model(X_test_tensor) > 0.5).float()\n",
    "    nn_test_acc = accuracy_score(y_test, test_preds.numpy())\n",
    "\n",
    "print(f\"Neural Network Test Accuracy: {nn_test_acc * 100:.2f}%\")\n",
    "print(\"Classification Report (Neural Network):\")\n",
    "print(classification_report(y_test, test_preds.numpy()))"
   ],
   "id": "df1773c2ae6d81c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Now, We no Longer have access to the training dataset",
   "id": "bf910e32789a78c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2: Generate synthetic data for training the surrogate model",
   "id": "30aeea58b3a86464"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "loaded_model = torch.load(\"neural_network_model.pth\", map_location=\"cpu\", weights_only=False)\n",
    "loaded_model.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "with open('scaler.pkl', 'rb') as f:\n",
    "    loaded_scaler = pickle.load(f)\n",
    "with open('feature_names.pkl', 'rb') as f:\n",
    "    loaded_feature_names = pickle.load(f)\n",
    "\n",
    "print(f\"Scaler and feature names loaded ({len(loaded_feature_names)} features)\")\n",
    "\n",
    "num_synthetic_samples = 30000\n",
    "num_features = len(loaded_feature_names)\n",
    "\n",
    "synthetic_data_scaled = []\n",
    "\n",
    "print(\"Generating synthetic data for surrogate model training\")\n",
    "uniform_samples = np.random.uniform(-3, 3, size=(num_synthetic_samples//3, num_features))\n",
    "normal_samples = np.random.randn(num_synthetic_samples//3, num_features)\n",
    "\n",
    "lhs_sampler = qmc.LatinHypercube(d=num_features, seed=42)\n",
    "lhs_samples = lhs_sampler.random(n=num_synthetic_samples//3)\n",
    "lhs_samples = lhs_samples * 6 - 3\n",
    "\n",
    "synthetic_data_scaled.extend([uniform_samples, normal_samples, lhs_samples])\n",
    "X_synthetic_scaled = np.vstack(synthetic_data_scaled)\n",
    "print(f\"Generated {X_synthetic_scaled.shape[0]} synthetic samples\")"
   ],
   "id": "b9790d4bdd9ea071",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Get Neural Network's prediction on Synthetic data",
   "id": "397038ef2ed16d16"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Getting neural network predictions on synthetic data\")\n",
    "X_synth_tensor = torch.tensor(X_synthetic_scaled, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_synth = (loaded_model(X_synth_tensor) > 0.5).numpy().astype(int).flatten()\n",
    "\n",
    "unique, counts = np.unique(y_synth, return_counts=True)\n",
    "print(f\"Synthetic labels generated: {dict(zip(unique, counts))}\")\n"
   ],
   "id": "5643c95b58206ca1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 3: Train Surrogate model on synthetic data",
   "id": "da17b42b89ca25b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Training decision tree surrogate model on synthetic data...\")\n",
    "tree = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "tree.fit(X_synthetic_scaled, y_synth)\n",
    "\n",
    "tree_pred = tree.predict(X_synthetic_scaled)\n",
    "fidelity_synth = accuracy_score(y_synth, tree_pred)\n",
    "print(f\"Decision tree trained! Fidelity on synthetic data: {fidelity_synth * 100:.2f}%\")\n",
    "\n",
    "with open('decision_tree.pkl', 'wb') as f:\n",
    "    pickle.dump(tree, f)\n",
    "print(\"Decision tree saved to 'decision_tree.pkl'\")\n",
    "\n",
    "rules = export_text(tree, feature_names=loaded_feature_names, decimals=3)\n",
    "print(\"Extracted Decision Rules:\\n\")\n",
    "print(rules)"
   ],
   "id": "721d53f07fac05c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 4: Generate Manual Decision Rule Classifier\n",
    "## which will be a simple python function and store it in a python file"
   ],
   "id": "3bf161b9bf425fe5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def extract_manual_rules(tree, feature_names, scaler):\n",
    "    tree_ = tree.tree_\n",
    "    feature_name = [\n",
    "        feature_names[i] if i != -2 else \"undefined!\"\n",
    "        for i in tree_.feature\n",
    "    ]\n",
    "\n",
    "    rules_code = []\n",
    "    rules_code.append(\"def classify(features):\")\n",
    "    rules_code.append(\"    # features should be a numpy array of shape (n_features,)\")\n",
    "    rules_code.append(\"    # Scaled using mean and std from training\")\n",
    "    rules_code.append(f\"    mean = {scaler.mean_.tolist()}\")\n",
    "    rules_code.append(f\"    std = {scaler.scale_.tolist()}\")\n",
    "    rules_code.append(\"    \")\n",
    "    rules_code.append(\"    # Normalize features\")\n",
    "    rules_code.append(\"    X = [(features[i] - mean[i]) / std[i] for i in range(len(features))]\")\n",
    "    rules_code.append(\"    \")\n",
    "\n",
    "    def recurse(node, depth):\n",
    "        indent = \"    \" * (depth + 1)\n",
    "        if tree_.feature[node] != -2:\n",
    "            name = feature_name[node]\n",
    "            threshold = tree_.threshold[node]\n",
    "            feature_idx = tree_.feature[node]\n",
    "\n",
    "            rules_code.append(f\"{indent}if X[{feature_idx}] <= {threshold:.10f}:  # {name}\")\n",
    "            recurse(tree_.children_left[node], depth + 1)\n",
    "            rules_code.append(f\"{indent}else:  # {name} > {threshold:.10f}\")\n",
    "            recurse(tree_.children_right[node], depth + 1)\n",
    "        else:\n",
    "            value = tree_.value[node]\n",
    "            class_pred = int(value[0][1] > value[0][0])\n",
    "            rules_code.append(f\"{indent}return {class_pred}\")\n",
    "\n",
    "    recurse(0, 0)\n",
    "\n",
    "    return \"\\n\".join(rules_code)\n",
    "\n",
    "manual_rules_code = extract_manual_rules(tree, loaded_feature_names, loaded_scaler)\n",
    "print(\"Classify Function in python style:\\n\")\n",
    "print(manual_rules_code)\n",
    "\n",
    "with open('manual_rules.py', 'w') as f:\n",
    "    f.write(manual_rules_code)\n",
    "print(\"Manual rules saved to 'manual_rules.py'\")"
   ],
   "id": "942aa24bda7430dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create an executable of this function for future use",
   "id": "8b9d98127003774e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classify_globals = {}\n",
    "exec(manual_rules_code, classify_globals)\n",
    "classify = classify_globals['classify']"
   ],
   "id": "7986019a747fb156",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 4: Evaluate the Decision Tree and Manual Decision Rules",
   "id": "380c5390df53d291"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tree_test_preds = tree.predict(X_test_scaled)\n",
    "tree_test_acc = accuracy_score(y_test, tree_test_preds)\n",
    "\n",
    "manual_test_preds = np.array([classify(X_test[i]) for i in range(len(X_test))])\n",
    "manual_test_acc = accuracy_score(y_test, manual_test_preds)\n",
    "\n",
    "nn_test_preds = (loaded_model(X_test_tensor) > 0.5).numpy().astype(int).flatten()\n",
    "fidelity_tree = accuracy_score(nn_test_preds, tree_test_preds)\n",
    "fidelity_manual = accuracy_score(nn_test_preds, manual_test_preds)\n",
    "\n",
    "print(\"Decision Tree Test Accuracy:\", tree_test_acc)\n",
    "print(\"Manual Rules Test Accuracy:\", manual_test_acc)\n",
    "print()\n",
    "print(\"Fidelity Tree:\", fidelity_tree)\n",
    "print(\"Fidelity Manual:\", fidelity_manual)"
   ],
   "id": "8290f7e4af314590",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Measure performance",
   "id": "2a5a2753ccf1b4db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def to_binary_class(pred):\n",
    "    if isinstance(pred, torch.Tensor):\n",
    "        arr = pred.squeeze().detach().cpu().numpy()\n",
    "        pred = arr\n",
    "    if isinstance(pred, np.ndarray):\n",
    "        if pred.size == 1:\n",
    "            return int(float(pred.item()) >= 0.5)\n",
    "        return int(np.argmax(pred))\n",
    "    if isinstance(pred, (list, tuple)):\n",
    "        arr = np.array(pred)\n",
    "        if arr.size == 1:\n",
    "            return int(float(arr.item()) >= 0.5)\n",
    "        return int(np.argmax(arr))\n",
    "    try:\n",
    "        return int(float(pred) >= 0.5)\n",
    "    except:\n",
    "        return int(pred)\n",
    "\n",
    "try:\n",
    "    model_device = next(loaded_model.parameters()).device\n",
    "except:\n",
    "    model_device = torch.device(\"cpu\")\n",
    "\n",
    "rows = []\n",
    "num_samples = len(X_test)\n",
    "\n",
    "for i in range(num_samples):\n",
    "    x_single = X_test_scaled[i:i+1]\n",
    "    xt = torch.tensor(x_single, dtype=torch.float32).to(model_device)\n",
    "    raw = X_test[i]\n",
    "\n",
    "    # ---- Neural Network ----\n",
    "    gc.collect()\n",
    "    tracemalloc.start()\n",
    "    t0 = time.perf_counter_ns()\n",
    "    with torch.no_grad():\n",
    "        nn_out = loaded_model(xt)\n",
    "    nn_time_ms = (time.perf_counter_ns() - t0) / 1e6\n",
    "    _, nn_peak = tracemalloc.get_traced_memory()   # in bytes\n",
    "    tracemalloc.stop()\n",
    "    nn_mem_kb = nn_peak / 1024.0\n",
    "    nn_class = to_binary_class(nn_out)\n",
    "\n",
    "    # ---- Tree Classifier ----\n",
    "    gc.collect()\n",
    "    tracemalloc.start()\n",
    "    t0 = time.perf_counter_ns()\n",
    "    tree_pred = tree.predict(x_single)\n",
    "    tree_time_ms = (time.perf_counter_ns() - t0) / 1e6\n",
    "    _, tree_peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    tree_mem_kb = tree_peak / 1024.0\n",
    "    tree_class = to_binary_class(tree_pred)\n",
    "\n",
    "    # ---- Manual Classifier ----\n",
    "    gc.collect()\n",
    "    tracemalloc.start()\n",
    "    t0 = time.perf_counter_ns()\n",
    "    manual_pred = classify(raw)\n",
    "    manual_time_ms = (time.perf_counter_ns() - t0) / 1e6\n",
    "    _, manual_peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    manual_mem_kb = manual_peak / 1024.0\n",
    "    manual_class = to_binary_class(manual_pred)\n",
    "\n",
    "    # Speedup: how many times faster manual is vs NN = NN_time / Manual_time\n",
    "    if manual_time_ms and manual_time_ms > 0:\n",
    "        speedup = nn_time_ms / manual_time_ms\n",
    "    else:\n",
    "        speedup = np.nan\n",
    "\n",
    "    # Memory improvement of manual wrt NN expressed as percentage: (NN - Manual) / NN * 100\n",
    "    if nn_mem_kb and nn_mem_kb > 0:\n",
    "        mem_impr_pct = (nn_mem_kb - manual_mem_kb) / nn_mem_kb * 100.0\n",
    "    else:\n",
    "        mem_impr_pct = np.nan\n",
    "\n",
    "    rows.append({\n",
    "        \"sample\": i,\n",
    "        \"neural_network_classification\": nn_class,\n",
    "        \"neural_network_execution_time_ms\": nn_time_ms,\n",
    "        \"neural_network_memory_kb\": nn_mem_kb,\n",
    "        \"tree_classification\": tree_class,\n",
    "        \"tree_execution_time_ms\": tree_time_ms,\n",
    "        \"tree_memory_kb\": tree_mem_kb,\n",
    "        \"manual_traversal_classification\": manual_class,\n",
    "        \"manual_traversal_execution_time_ms\": manual_time_ms,\n",
    "        \"manual_traversal_memory_kb\": manual_mem_kb,\n",
    "        \"speedup_nn_over_manual\": speedup,\n",
    "        \"memory_improvement_manual_vs_nn_pct\": mem_impr_pct\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(rows)"
   ],
   "id": "f661c98e436caa91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_df = results_df.rename(columns={\n",
    "    \"sample\": \"Sample\",\n",
    "    \"neural_network_classification\": \"NN Classification\",\n",
    "    \"neural_network_execution_time_ms\": \"NN Execution time (ms)\",\n",
    "    \"neural_network_memory_kb\": \"NN Memory required (KB)\",\n",
    "    \"tree_classification\": \"Tree Classification\",\n",
    "    \"tree_execution_time_ms\": \"Tree Execution time (ms)\",\n",
    "    \"tree_memory_kb\": \"Tree Memory required (KB)\",\n",
    "    \"manual_traversal_classification\": \"Manual Classification\",\n",
    "    \"manual_traversal_execution_time_ms\": \"Manual Execution time (ms)\",\n",
    "    \"manual_traversal_memory_kb\": \"Manual Memory required (KB)\",\n",
    "    \"speedup_nn_over_manual\": \"Speedup (NN/Manual)\",\n",
    "    \"memory_improvement_manual_vs_nn_pct\": \"Memory improvement vs NN (%)\"\n",
    "})\n",
    "\n",
    "num_cols = [\n",
    "    \"NN Execution time (ms)\",\n",
    "    \"NN Memory required (KB)\",\n",
    "    \"Tree Execution time (ms)\",\n",
    "    \"Tree Memory required (KB)\",\n",
    "    \"Manual Execution time (ms)\",\n",
    "    \"Manual Memory required (KB)\",\n",
    "    \"Speedup (NN/Manual)\",\n",
    "    \"Memory improvement vs NN (%)\"\n",
    "]\n",
    "table_df[num_cols] = table_df[num_cols].astype(float).round(5)\n",
    "display(table_df.sample(40, random_state=42).reset_index(drop=True))"
   ],
   "id": "aa9ba42b336e8131",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Average improvement in memory: \",np.mean(results_df[\"memory_improvement_manual_vs_nn_pct\"]),  \"%\")\n",
    "print(\"Average speedup in time: \",np.mean(results_df[\"speedup_nn_over_manual\"]), \"times\")"
   ],
   "id": "38f6f93ba07cfab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Print Performance and Size Statistics",
   "id": "c4498dd13d3bcaec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fmt(x):\n",
    "    try:\n",
    "        return float(f\"{float(x):.5f}\")\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "mean_nn_time = results_df[\"neural_network_execution_time_ms\"].mean()\n",
    "mean_tree_time = results_df[\"tree_execution_time_ms\"].mean()\n",
    "mean_manual_time = results_df[\"manual_traversal_execution_time_ms\"].mean()\n",
    "\n",
    "mean_nn_mem = results_df[\"neural_network_memory_kb\"].mean()\n",
    "mean_tree_mem = results_df[\"tree_memory_kb\"].mean()\n",
    "mean_manual_mem = results_df[\"manual_traversal_memory_kb\"].mean()\n",
    "\n",
    "mean_speedup = results_df[\"speedup_nn_over_manual\"].mean()\n",
    "mean_mem_impr_pct = results_df[\"memory_improvement_manual_vs_nn_pct\"].mean()\n",
    "\n",
    "print(\"NN Mean Time (ms):\", fmt(mean_nn_time))\n",
    "print(\"Tree Mean Time (ms):\", fmt(mean_tree_time))\n",
    "print(\"Manual Mean Time (ms):\", fmt(mean_manual_time))\n",
    "print()\n",
    "\n",
    "print(\"NN Mean Memory (KB):\", fmt(mean_nn_mem))\n",
    "print(\"Tree Mean Memory (KB):\", fmt(mean_tree_mem))\n",
    "print(\"Manual Mean Memory (KB):\", fmt(mean_manual_mem))\n",
    "print()\n",
    "\n",
    "print(\"Mean Speedup (NN/Manual):\", fmt(mean_speedup))\n",
    "print(\"Mean Memory improvement of Manual vs NN (%):\", fmt(mean_mem_impr_pct))\n",
    "print()\n",
    "\n",
    "# file sizes (KB)\n",
    "nn_size = os.path.getsize('neural_network_model.pth') / 1024 if os.path.exists('neural_network_model.pth') else np.nan\n",
    "tree_size = os.path.getsize('decision_tree.pkl') / 1024 if os.path.exists('decision_tree.pkl') else np.nan\n",
    "manual_size = os.path.getsize('manual_rules.py') / 1024 if os.path.exists('manual_rules.py') else np.nan\n",
    "\n",
    "print(\"NN File Size (KB):\", fmt(nn_size))\n",
    "print(\"Tree File Size (KB):\", fmt(tree_size))\n",
    "print(\"Manual File Size (KB):\", fmt(manual_size))"
   ],
   "id": "98f44c1594f6a0a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Thank You",
   "id": "642f05f1642c2451"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
